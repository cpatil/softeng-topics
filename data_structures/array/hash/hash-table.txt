Hash Tables

Algorithm	Average		Worst case
-----------------------------------
Space 		O(n)		O(n)
Search 		O(1)		O(n)
Insert		O(1)		O(n)
Delete		O(1)		O(n)

Is a data structure that implements an associative array abstract data type, a structure that can map keys to values. Uses a hash function to compute an index, also called a hash code, into an array of buckets or slots, from which the desired value can be found. During lookup, the key is hashed and the resulting hash indicates where the corresponding value is stored

Ideally, the hash function will asign each key to a unique bucket, but most hash table designs employ an imperfect hash function, which might cause hash collisions where the hash function generates the same index for more than one key.

In a well-dimensioned hash table, the average cost (number of instructions) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow aribtrary insertions and deletions of key-value pairs, at constant average cost per operations

In many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets

The idea of hashing is to distribute the entries (key/value pairs) acress an array of buckets. Given a key, the algorithm computes an index that suggests where the entry can be found:

					index = f(key, array_size)
								
					hash = hashfunc(key)
					index = hash % array_size

In the second method the hash is independent of the array size, and it is then reduced to an index (a number between 0 and array_size - 1 using the modulo operator (%): returns the remainder or signed remainder of a division, after one number is divided by another

In the case that the array size is a power of two, the remainder operation is reduced to masking, which improves speed, but can increase problems with a poor hash function

Bit Masking: A mask or bitmask is data that is used for bitwise operations, particularly in a bit field. Using a mask, multiple bits in a bute, nibble, word etc. can be set either on, off or inverted from on to off ( or vice versa) in a single bitwise operation

Choosing a hash function:
A basic requirement is that the function should provude of uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them

Uniform distribution:
In probability theory and statistics, the discrete uniform distribution is a symmetric probability distribution wherein a finite number of values are equally likely to be observed; every one of n values has equal probability 1/n. A simple example of the discrete uniform distribution is throwing a fair dice. The possible values are 1, 2, 3, 4, 5, 6, and each time the die is thrown the probability of a given score is 1/6. 

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent.

For cryptographic hash functions, they're believed to provide good hash functions for any table size, either by modulo reduction or by bit masking. A draw back is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable

The idea cryptographic hash function has the following main properties:
	- It is deterministic, meaning that the same message always results in the same hash
	- It is quick to compute the hash value for any given message
	- It is infeasible to generate a message that yields a given hash value (i.e. to reverse the process is also known that generated the given hash value)
	- It is infeasible to find two different messages with the same hash value
	- A small change to a message should change the hash value so extensively that a new hash value appears uncorrelated with the old hash value (avalanche effect)

For K-independent hashing, it offers a way to prove a certain hash function doesn't have bad keysets for a given type of hashtable. A number of such results are known for collision resolution schemes such as linear probing and cuckoo hashing. Since K-independence can prove a hash function works, one can then focus on finding the fastest possible such hash function
	- A family of hash functions is said to be k-independent, k-wise independent or k-universal if selecting a function at random from the family guarantees that the hash codes of any designated k keys are independent random variables. Such families allow good average case performance in randomized algorithms or data structures, even if the input data is chosen by an adversary. The trade-offs between the degree of independence and the efficiency of evaluating the hash function are well studied, and many k-independent families have been proposed

If all keys are known ahead of time, a perfect hash function can be used to create a perfect hash table that has no collisions. If minimal perfect hasing is used, every location in the hash table can be used as well. Perfect hashing allows for constant time lookups in all cases. This is in contrast to most chaining and open addressing methods, where the time for lookup is low on average, but may be very large, O(n), for instance when all the keys hash to a few values

Consistent hashing:
Is a special kind of hashing such that when a hash table is resized, only n/m keys need to be remapped on average where n is the number of keys and m  is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and the slots is defined by a modular operation. 

Rendezvous hashing:
Rendezvous or highest random weight (HRW) hashing is an algorithm that allows clients to achieve distributed agreement on a set of k options out of a possible set of n options. A typical application is when clients need to agree on which sites (or proxies) objects are assigned to. Rendezvous hashing is both much simpler and more general than consistent hashing, which becomes a special case (for k = 1) of rendezvous hashing. 

Load Factor: LF = n/k', where n is the number of entries occupied in the hash table and k is the number of buckets

Linear hashing
Permits incremental hash table expansion. It is implemented using a single hash table, but with two possible lookup functions.

Hashing for distributed hash tables
Another way to decrease the cost of table resizing is to choose a hash function in such a way that the hashes of most values do not change when the table is resized. Such hash functions are prevalent in disk-based and distributed hash tables, where rehashing is prohibitively costly. The problem of designing a hash such that most values do not change when the table is resized is known as the distributed hash table problem. The four most popular approaches are rendezvous hashing, consistent hashing, the content addressable network algorithm, and Kademlia distance. 

Universal hashing:
Refers to selecting a hash function at random from a family of hash functions with a certain mathematical property (see definition below). This guarantees a low number of collisions in expectation, even if the data is chosen by an adversary. Many universal families are known (for hashing integers, vectors, strings), and their evaluation is often very efficient. Universal hashing has numerous uses in computer science, for example in implementations of hash tables, randomized algorithms, and cryptography. 