First consider the definitions below:

Space Complexity = Auxiliary Space + Input space

Auxiliary Space

is the temporary space allocated by your algorithm to solve the problem, with
respect to input size.

Space Complexity is the total space used by your algorithm to solve the problem,
with respect to input size. Note that the Space Complexity includes

input size.

I can see where your confusion comes from; usually, when we say "Space
complexity of Mergesort" we mean its Auxiliary Space, because we obviously
cannot improve the space we use for the input itself. As such, some of the
statements in your question are not correct.

âˆ™ Mergesort has Auxiliary space of O(n) (in its common use) since you allocate a
new array, and Space complexity of O(n)

An example between the two would be:

If we compare Quicksort and Mergesort, they both have a space complexity of O(n)
and run at O(nlogn) time, but Mergesort requires Auxiliary space of O(n) while
Quicksort requires Auxiliary space of O(1)

Finally, to answer your question: usually the intention while asking about
"space complexity" is indeed Auxiliary space, although there is some ambiguity
here in regard to the formal definitions above. To be pedantic, you can separate
the two space complexity types in your answer.

Space Complexity refers to the magnitude of auxiliary space your program takes to process the input.

There are broadly two kinds of algorithms we have to calculate the space complexity for:

    Iterative Algorithms

    For iterative algorithms we have to consider the variables and data
    structures we declare in our program. To be more general, if we reserve any
    memory apart from the input, it counts towards the space complexity.

    For example declaring an array of size n would add to the space complexity
    by a factor of O(n), a 2D array would add to the space complexity by a
    factor of O(n^2) and so on. Simple variables add a space complexity of O(1)
    since the size of a particular datatype is always constant. The more
    trickier cases are the user defined datatypes, however in most cases
    iterative algorithms are straightforward to analyse in terms of space
    complexity. Recursive Algorithms

    These algorithms are trickier because in addition to taking care of the
    variables and data structures (memory reserved for operations in the heap),
    we also have to take care of the stack memory which is used for recursive
    calls.

    For example, in the Fibonacci algorithm we have F(n) = F(n-1) + F(n-2), n>2.
    If we trace out the recursive calls we see that F(4) calls F(3) and F(2).
    F(3) calls F(2) and F(1) which return base cases. So at a given point of
    time we see that the program does not occupy more than n cells in the stack
    memory for a function call of F(n). This will be much easier to understand
    if you draw the function calls on a piece of paper and push each driver onto
    the stack before descending to a lower level of recursion.

    Hence we can safely state that for this particular algo, we have a space
    complexity of O(n) (assuming that each cell in the stack reserves equal
    space).